{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'myutils' from '/home/antoniovs/Escolar/Sem09/anal-txts/proyecto2/myutils.py'>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pickle\n",
    "import myutils\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "\n",
    "from importlib import reload\n",
    "reload(myutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando y Serializando el Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    text = \"\"\n",
    "    with open(path, \"r\") as inputf:\n",
    "        for line in inputf:\n",
    "            text += line.lower()\n",
    "    return text\n",
    "\n",
    "def load_texts(path, lang):\n",
    "    movieids = []\n",
    "    texts = []\n",
    "    for label in [\"0/\", \"1/\", \"2/\"]:\n",
    "        movieids_genero = [] \n",
    "        texts_genero = []\n",
    "        directory = os.path.join(path, label)\n",
    "        for txtfile in sorted(os.listdir(directory)):\n",
    "            if txtfile.endswith(\".\" + lang + \".txt\"):\n",
    "                movieids_genero.append(txtfile.split(\".\")[0])\n",
    "                texts_genero.append(load_text(os.path.join(directory, txtfile)))\n",
    "        texts.append(texts_genero)\n",
    "        movieids.append(movieids_genero)\n",
    "    return movieids, texts\n",
    "\n",
    "def cargar_titulos(path):\n",
    "    titulos = dict()\n",
    "    with open(path, \"r\") as inputf:\n",
    "        for line in inputf:\n",
    "            titulo_id = line.split(\" \")[0]\n",
    "            titulo = line.split(\"  \")[1].rstrip()\n",
    "            titulos[titulo_id] = titulo\n",
    "    return titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieids, txts_eng = load_texts(\"clean_dataset/\", \"eng\")\n",
    "movieids, txts_spa = load_texts(\"clean_dataset/\", \"spa\")\n",
    "movieids, txts_fre = load_texts(\"clean_dataset/\", \"fre\") # los movieids son iguales para las 3 lenguas\n",
    "\n",
    "titulos_accion = cargar_titulos(\"clean_dataset/0_accion.txt\")\n",
    "titulos_romance = cargar_titulos(\"clean_dataset/1_romance.txt\")\n",
    "titulos_horror = cargar_titulos(\"clean_dataset/2_horror.txt\")\n",
    "titulos = [titulos_accion, titulos_romance, titulos_horror]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo: Subtítulos de 'Gremlins (1984)'\n",
      "\n",
      "En Inglés: \n",
      " friend let me introduce myself peltzer the name rand peltzer that be me there on the corner be inventor and have story to tell know who have not get story well nobody get story like this nobody it all start here in chinatown be try to move little merchandise maybe find present for my kid try this pl \n",
      "\n",
      "En Español:\n",
      " amigo dejar que me presente me llamar peltzer rand peltzer ser ese de el esquina ser inventor tener uno historia que contar el saber todo tener historia pero nadie tener uno como este nadie todo empezar aquí en chinatown tratar de vender alguno mercancía encontrar uno regalo para mi hijo probar en u \n",
      "\n",
      "En Francés:\n",
      " mon ami permettre de me prêsenter rand peltzer ce être moi là le coin je suivre inventeur et je avoir un histoire raconter je savoir tout le monde avoir un histoire mais personne en avoir un comme celui ci personne tout avoir commencê ici chinatown je essayer de placer mon produit et de trouver un c \n",
      "\n"
     ]
    }
   ],
   "source": [
    "genero = 2 # 0 para accion, 1 para romance y 2 para horror\n",
    "peli = 10\n",
    "\n",
    "print(\"Ejemplo: Subtítulos de '{0}'\\n\".format(titulos[genero][movieids[genero][peli]]))\n",
    "print(\"En Inglés: \\n\", txts_eng[genero][peli][:300], \"\\n\")\n",
    "print(\"En Español:\\n\", txts_spa[genero][peli][:300], \"\\n\")\n",
    "print(\"En Francés:\\n\", txts_fre[genero][peli][:300], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(movieids, open(\"./pickles/movieids.pickle\", \"wb\"))\n",
    "pickle.dump(titulos, open(\"./pickles/titulos.pickle\", \"wb\"))\n",
    "\n",
    "pickle.dump(txts_spa, open(\"./pickles/txts_spa.pickle\", \"wb\"))\n",
    "pickle.dump(txts_eng, open(\"./pickles/txts_eng.pickle\", \"wb\"))\n",
    "pickle.dump(txts_fre, open(\"./pickles/txts_fre.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos de peliculas:\n",
      " [('0468569', 'Batman: El Caballero de la Noche (2008)'), ('1375666', 'El origen (2010)'), ('0133093', 'Matrix (1999)'), ('0167260', 'El señor de los anillos - El retorno del rey (2003)'), ('1345836', 'Batman: El caballero de la noche asciende (2012)'), ('0172495', 'Gladiador (2000)'), ('0372784', 'Batman inicia (2005)'), ('0848228', 'The Avengers: Los Vengadores (2012)'), ('0076759', 'La guerra de las galaxias (1977)'), ('0080684', 'El imperio contraataca (1980)'), ('0499549', 'Avatar (2009)'), ('0325980', 'Piratas del Caribe - La maldición del Perla Negra (2003)'), ('0434409', 'V de Venganza (2005)'), ('2015381', 'Guardianes de la galaxia (2014)'), ('0103064', 'Terminator 2: Juicio Final (1991)')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ejemplos de peliculas:\\n\", list(titulos[0].items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorando los géneros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Pelis de acción y romance -\n",
      "\n",
      "- Pelis de acción y horror -\n",
      "Guerra mundial Z (2013)\n",
      "\n",
      "- Pelis de romance y horror -\n",
      "Mi novio es un zombie (2013)\n"
     ]
    }
   ],
   "source": [
    "print(\"- Pelis de acción y romance -\")\n",
    "s = set(movieids[0]) & set(movieids[1])\n",
    "for movieid in s:\n",
    "    print(titulos[0][movieid])\n",
    "\n",
    "print(\"\\n- Pelis de acción y horror -\")\n",
    "s = set(movieids[0]) & set(movieids[2])\n",
    "for movieid in s:\n",
    "    print(titulos[0][movieid])\n",
    "\n",
    "print(\"\\n- Pelis de romance y horror -\")\n",
    "s = set(movieids[1]) & set(movieids[2])\n",
    "for movieid in s:\n",
    "    print(titulos[1][movieid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando y Serializando Diccionarios id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_spa = corpora.Dictionary([txt.split(\" \") for txt in txts_spa[0] + txts_spa[1] + txts_spa[2]])\n",
    "dictionary_eng = corpora.Dictionary([txt.split(\" \") for txt in txts_eng[0] + txts_eng[1] + txts_eng[2]])\n",
    "dictionary_fre = corpora.Dictionary([txt.split(\" \") for txt in txts_fre[0] + txts_fre[1] + txts_fre[2]])\n",
    "\n",
    "dictionary_spa.save('pickles/dictionary_spa.dict')\n",
    "dictionary_eng.save('pickles/dictionary_eng.dict')\n",
    "dictionary_fre.save('pickles/dictionary_fre.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "el\n"
     ]
    }
   ],
   "source": [
    "print(dictionary_spa.token2id['el'])\n",
    "print(dictionary_spa[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(445, 1), (870, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_spa.doc2bow([\"el\", \"no\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# de tokens en Inglés: 32935\n",
      "# de tokens en Español: 32710\n",
      "# de tokens en Francés: 32044\n"
     ]
    }
   ],
   "source": [
    "print(\"# de tokens en Inglés:\", len(dictionary_eng))\n",
    "print(\"# de tokens en Español:\", len(dictionary_spa))\n",
    "print(\"# de tokens en Francés:\", len(dictionary_fre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obteniendo Frecuencias del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_frecuencias(txts):\n",
    "    frecs = []\n",
    "    for genero in txts:\n",
    "        contador = Counter()\n",
    "        for pelicula in genero:\n",
    "            for word in pelicula.split(\" \"):\n",
    "                contador[word] += 1\n",
    "        frecs.append(contador)\n",
    "    return frecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frecs_spa = obtener_frecuencias(txts_spa)\n",
    "frecs_eng = obtener_frecuencias(txts_eng)\n",
    "frecs_fre = obtener_frecuencias(txts_fre)\n",
    "\n",
    "pickle.dump(frecs_spa, open(\"./pickles/frecs_spa.pickle\", \"wb\"))\n",
    "pickle.dump(frecs_eng, open(\"./pickles/frecs_eng.pickle\", \"wb\"))\n",
    "pickle.dump(frecs_fre, open(\"./pickles/frecs_fre.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palabras más comunes del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Palabras más frecuentes en Español:\n",
      "[('el', 53676), ('de', 25243), ('que', 22083), ('ser', 21184), ('no', 20405), ('uno', 16306), ('estar', 11579), ('en', 10408), ('haber', 8229), ('qué', 7103)]\n",
      "\n",
      "10 Palabras más frecuentes en Inglés:\n",
      "[('be', 51031), ('you', 34179), ('the', 27981), ('to', 19030), ('it', 15140), ('not', 15120), ('do', 14096), ('that', 10786), ('have', 10370), ('of', 10369)]\n",
      "\n",
      "10 Palabras más frecuentes en Francés:\n",
      "[('le', 52881), ('de', 32719), ('être', 29210), ('je', 22542), ('avoir', 21538), ('ce', 16609), ('un', 15930), ('que', 15124), ('ne', 12609), ('pas', 12337)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genero = 0\n",
    "maxp = 10\n",
    "\n",
    "print(\"{0} Palabras más frecuentes en Español:\\n{1}\\n\".format(maxp, frecs_spa[genero].most_common(maxp)))\n",
    "print(\"{0} Palabras más frecuentes en Inglés:\\n{1}\\n\".format(maxp, frecs_eng[genero].most_common(maxp)))\n",
    "print(\"{0} Palabras más frecuentes en Francés:\\n{1}\\n\".format(maxp, frecs_fre[genero].most_common(maxp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('el', 140421), ('de', 68722), ('ser', 64269), ('no', 63435), ('que', 62450), ('uno', 47769), ('estar', 35530), ('en', 29297), ('qué', 23571), ('haber', 21880), ('tener', 21429), ('me', 20681), ('por', 19809), ('ir', 19651), ('te', 17670), ('hacer', 17515), ('ese', 17064), ('poder', 16701), ('este', 16488), ('sí', 14156), ('se', 13783), ('decir', 13394), ('con', 13343), ('mi', 13219), ('saber', 12805), ('querer', 12478), ('para', 11893), ('bien', 11563), ('todo', 10744), ('ver', 10609), ('su', 10484), ('pero', 10418), ('yo', 10164), ('si', 9686), ('tu', 9203), ('aquí', 8096), ('le', 7966), ('más', 7415), ('bueno', 7308), ('él', 7102), ('como', 6887), ('creer', 6773), ('ya', 6544), ('muy', 5793), ('tú', 5742), ('dar', 5622), ('deber', 5559), ('pasar', 5498), ('algo', 5052), ('ahora', 4790), ('nos', 4783), ('cómo', 4699), ('así', 4566), ('dejar', 4447), ('nada', 4254), ('mucho', 4207), ('vez', 4134), ('gracia', 4015), ('esperar', 3933), ('cuando', 3881), ('hablar', 3831), ('otro', 3720), ('sólo', 3530), ('dios', 3521), ('necesitar', 3433), ('sentar', 3432), ('llamar', 3297), ('cosa', 3253), ('venir', 3236), ('quién', 3148), ('oír', 3074), ('mirar', 3069), ('favor', 3061), ('pensar', 3021), ('solo', 2982), ('llevar', 2907), ('usted', 2832), ('verdad', 2780), ('mí', 2693), ('tiempo', 2665), ('vida', 2620)]\n"
     ]
    }
   ],
   "source": [
    "print(myutils.set2wordfrecs(myutils.intersect_most_common(frecs_spa), frecs_spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('le', 136645), ('de', 88441), ('être', 86971), ('je', 74507), ('avoir', 61358), ('ce', 50981), ('un', 45723), ('que', 43430), ('pas', 37506), ('ne', 34484), ('tu', 30185), ('vous', 29588), ('il', 24876), ('et', 24461), ('aller', 22023), ('faire', 21258), ('on', 20963), ('en', 19643), ('cela', 18929), ('te', 18258), ('me', 17712), ('mon', 16019), ('pour', 13882), ('tout', 12775), ('se', 11909), ('pouvoir', 11831), ('dire', 11445), ('qui', 10471), ('mais', 10243), ('vouloir', 10209), ('savoir', 9762), ('dans', 9451), ('non', 9287), ('elle', 9218), ('suivre', 9204), ('nous', 9081), ('si', 8704), ('bien', 8681), ('ton', 8156), ('moi', 8117), ('plus', 7944), ('voir', 7717), ('oui', 7471), ('devoir', 7451), ('avec', 7220), ('son', 6986), ('quoi', 6702), ('votre', 6021), ('ils', 5982), ('là', 5970), ('sur', 5561), ('toi', 5359), ('comme', 5276), ('venir', 5061), ('ici', 4968), ('bon', 4802), ('où', 4664), ('rien', 4377), ('lui', 4260), ('croire', 4017), ('pourquoi', 3857), ('aimer', 3792), ('prendre', 3681), ('chose', 3658), ('parler', 3658), ('passer', 3550), ('notre', 3491), ('par', 3467), ('quand', 3463), ('merci', 3441), ('autre', 3440), ('comment', 3431), ('alors', 3335), ('très', 3212), ('trouver', 3177), ('faillir', 3161), ('ou', 3158), ('jamais', 3007), ('même', 2943), ('attendre', 2915), ('aussi', 2762), ('laisser', 2723), ('penser', 2712), ('deux', 2478)]\n"
     ]
    }
   ],
   "source": [
    "print(myutils.set2wordfrecs(myutils.intersect_most_common(frecs_fre), frecs_fre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentando el corpus (explicación)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hacer', 'mucho', 'tiempo', 'en', 'uno', 'galaxia', 'muy', 'muy', 'lejano', 'episodio', 'iv', 'una', 'nueva', 'esperanza', 'ser', 'uno', 'período', 'de', 'guerra', 'civil', 'el', 'nave', 'espacial', 'rebelde', 'estacionar', 'en', 'uno', 'base', 'oculto', 'haber', 'lograr', 'su', 'victoria', 'en', 'contra', 'de', 'el', 'malvado', 'imperio', 'galáctico', 'durante', 'el', 'batalla', 'el', 'espía', 'rebelde', 'se', 'el', 'haber', 'ingeniar', 'para', 'robar', 'el', 'plano', 'secreto', 'acerca', 'de', 'el', 'último', 'arma', 'de', 'el', 'imperio', 'denominar', 'estrella', 'de', 'la', 'muerte', 'uno', 'estación', 'espacial', 'blindar', 'con', 'el', 'suficiente', 'poder', 'como', 'para', 'destruir', 'uno', 'planeta', 'entero', 'perseguir', 'por', 'el', 'siniestro', 'agente', 'de', 'el', 'imperio', 'el', 'princesa', 'leia', 'se', 'dirigir', 'su', 'hogar', 'en', 'su', 'nave']\n"
     ]
    }
   ],
   "source": [
    "unfiltered_lists = myutils.texts2lists(txts_spa[0], stopwords = None)\n",
    "print(unfiltered_lists[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['galaxia', 'lejano', 'episodio', 'iv', 'una', 'nueva', 'esperanza', 'período', 'guerra', 'civil', 'nave', 'espacial', 'rebelde', 'estacionar', 'base', 'oculto', 'lograr', 'victoria', 'contra', 'malvado', 'imperio', 'galáctico', 'durante', 'batalla', 'espía', 'rebelde', 'ingeniar', 'robar', 'plano', 'secreto', 'acerca', 'último', 'arma', 'imperio', 'denominar', 'estrella', 'la', 'muerte', 'estación', 'espacial', 'blindar', 'suficiente', 'destruir', 'planeta', 'entero', 'perseguir', 'siniestro', 'agente', 'imperio', 'princesa', 'leia', 'dirigir', 'hogar', 'nave', 'espacial', 'custodiar', 'plano', 'robar', 'salvar', 'gente', 'restaurar', 'libertad', 'galaxia', 'alcanzar', 'reactor', 'principal', 'destruir', 'seguro', 'locura', 'perder', 'princesa', 'escapar', 'dónde', 'fin', 'dónde', 'meter', 'dirección', 'mandar', 'mina', 'kessel', 'triturar', 'momento', 'adónde', 'vas', 'plano', 'estrella', 'la', 'muerte', 'ordenador', 'central', 'dónde', 'transmisión', 'interceptar', 'plano', 'interceptar', 'ninguno', 'transmisión', 'nave', 'consular', 'misión']\n"
     ]
    }
   ],
   "source": [
    "stopwords = myutils.intersect_most_common(frecs_spa)\n",
    "filtered_lists = myutils.texts2lists(txts_spa[0], stopwords = stopwords)\n",
    "print(filtered_lists[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['galaxia', 'lejano', 'episodio', 'iv', 'una', 'nueva', 'esperanza', 'período', 'guerra', 'civil', 'nave', 'espacial', 'rebelde', 'estacionar', 'base', 'oculto', 'lograr', 'victoria', 'contra', 'malvado'] \n",
      "\n",
      "['oculto', 'lograr', 'victoria', 'contra', 'malvado', 'imperio', 'galáctico', 'durante', 'batalla', 'espía', 'rebelde', 'ingeniar', 'robar', 'plano', 'secreto', 'acerca', 'último', 'arma', 'imperio', 'denominar']\n"
     ]
    }
   ],
   "source": [
    "splitted_lists = myutils.split_lists(filtered_lists, chunk_size = 20, overlap_size = 5)\n",
    "print(splitted_lists[0], \"\\n\")\n",
    "print(splitted_lists[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 19176\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_lists), len(splitted_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(146, 1), (235, 1), (283, 1), (484, 1), (500, 1), (506, 1), (510, 1), (593, 1), (618, 1), (708, 1), (734, 1), (759, 1), (779, 1), (859, 1), (878, 1), (886, 1), (971, 1), (1072, 1), (1322, 1), (1355, 1)]\n"
     ]
    }
   ],
   "source": [
    "vectors = myutils.lists2bow(splitted_lists, dictionary_spa)\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
